HTML header: <title>dCache 4.1 Release Notes</title>
             <!--#include virtual="/template/default-head.shtml" -->
             <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
             <!-- Bootstrap -->
             <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"
                   integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous"/>
             <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css"
                   integrity="sha384-fLW2N01lMqjakBkx3l/M9EahuwpSfeNvV63J5ezn3uZzapT0u7EYsXMjQV+0En5r" crossorigin="anonymous"/>
             <!--link type="text/css" rel="stylesheet" href="jquery.tocify.css" /-->
             <style type="text/css">
             h5 {
                 font-size: 15px;
                 font-style: italic;
                 margin-top: 20px;
             }
             body {
                 padding-top: 20px;
             }
             @media (max-width: 767px) {
                 #toc {
                     position: relative;
                     width: 100%;
                     margin: 0px 0px 20px 0px;
                 }
             }
             @media print {
                 #toc {
                     display: none;
                 }
             }
             </style>

<div class="container">
<div id="banner">
   <div id="bird_large">
      <img src="/images/dcache-banner.png" class="bird"></img>
   </div>
   <div id="sidebar-b">
      <span class="dcache-nav-bar">
      <a href="/index.shtml">home</a>
      |
      <a href="/news.shtml">news</a>
      |
      <a href="/manuals/index.shtml">documentation</a>
      |
      <a href="/downloads/IAgree.shtml">downloads</a>
      |
      <a href="/feedback.shtml">feedback</a>
      |
      <a href="/manuals/googlesearch.shtml">search</a>
      |
      <a href="/imprint.shtml">imprint&#160;</a>
      </span>
   </div>
</div>
<div id="content">

<div class="row">

<div class="page-header">
<h1>What's new in dCache 4.1<br/>
<small>Release notes</small></h1>
</div>

## Highlights

This release of dCache introduces a new approach to external messaging. With the introduction
of Kafka as a message transport, for now in Billing, we start switching to a more modern and 
efficient messaging system. The goal for the Billing service is easier inclusion with 
systems like Elasticsearch: Instead of adding messages to a textual log and having an external
component parse that log, a Kafka-aware ingester can import them directly into the remote
system.

File transfers using GridFTP can now reuse TCP connections, which makes them faster and improves
compatibility with the Globus transfer service.

The global options to control publishing to kafka are added:
```
dcache.enable.kafka = true|false
dcache.kafka.bootstrap-servers= host1:port,host2:port
```
The kafka topic is **billing** and for not is not configurable. The record are written as JSON objects, like:
```
{
    "msgType": "request",
    "mappedUID": 0,
    "date": "Fri Mar 02 10:48:41 CET 2018",
    "cellName": "NFS-dcache-lab007",
    "VERSION": "1.0",
    "session": "door:NFS-dcache-lab007@core-dcache-lab007:AAVmauGezqg:1519984121401001",
    "fileSize": 969,
    "status": {
      "msg": "",
      "code": 0
    },
    "storageInfo": "size=969;new=true;stored=false;sClass=<Unknown>:<Unknown>;cClass=-;hsm=osm;accessLatency=NEARLINE;retentionPolicy=CUSTODIAL;uid=3750;gid=65534;flag-c=1:5ad529fc;",
    "cellType": "door",
    "cellDomain": "core-dcache-lab007",
    "mappedGID": 0,
    "billingPath": "/",
    "sessionDuration": 9,
    "queuingTime": 0,
    "pnfsid": "0000328CE70CBAF24579BF5AD373C05AECC0",
    "transferPath": "/",
    "@timestamp": "2018-03-02T09:48:41.412Z",
    "clientChain": "131.169.240.87",
    "type": "dcache_billing",
    "client": "131.169.240.87",
    "subject": [
      "UidPrincipal[0]",
      "GidPrincipal[0,primary]",
      "GidPrincipal[0]",
      "Origin[131.169.240.87]"
    ]
  }
```

The configuration of several components has been simplified. For example, creating LDAP-only setups
for gplazma now only requires the configuration of a single plugin, `ldap`. 

## Incompatibilities

(tbd, this section uses markdown formatting)
  - Starting from version 4.1 dCache head nodes are incompatible with pools older that 3.0.
  - NFSv4.1/pNFS is not compatible with older linux kernels when used with flexfile layout.
    As a result, by updating to dCache-4.1 a new export options ca be used to enforce layout
    driver. If you unsure about nfs client capabilities, export with **lt=fsv4_1_files**:
     ```
     / old-hosts(rw,lt=nfsv4_1_files)
     ```


## Acknowledgments

(tbd, this section uses markdown formatting)


## Release 4.1.0

### Admin

### Alarms

### Billing

A newly-introduced configuration property `dcache.enable.kafka` controls whether the Kafka messaging
system is used for message delivery.

### Cleaner

### DCAP

### Frontend

The 4.0 release saw the introduction of a range of RESTful services which
provide an administrative overview of the system.  There have been
a few minor changes in the intervening releases. The API is now stable with 4.1.

To summarize:  these services provide overviews of file data,
poolmanager configuration, service state, pool configuration and activity,
and system alarms.

One can similarly obtain full information on individual files by pnfsid;
billing records for reads, writes, p2ps, stores, restores and removes of a
given file are also available if your system is configured to use the billing
database.  With the database also comes histogram data for tracking reads,
writes, p2ps, stores and restores over intervals of 24 hours, 1 week, 1 month
and 1 year.

Histogram data on pool queues and file lifetime is similarly aggregated over
pools and pool groups.  This data collection is actually done by the history
service (introduced in 3.2), with which the frontend communicates,
so it is only available if the former has been deployed.

Finally, information corresponding roughly to the poolmanager.conf file is
furnished by the selection service API.

With 4.1 also comes integration with Swagger.  All RESTful services have been
provided with basic annotations in order automatically to generate API documentation.
A convenient web interface which allows exploration and testing of the API,
describing paths, parameters, error codes and JSON output, now runs at:

```
https://[host]:3880/api/v1.
```

Consulting this page is the easiest way to familiarize yourself with the API.

A final addition to this array of admin services, the space management service,
which will appear in 4.2, will provide link group and space token information.

A number of properties have been added to the defaults for the frontend, most
of which have to do with communication with other services (timeouts, endpoints,
etc.).  Consult the frontend.properties file for details.

### FTP

GridFTP defines a set of different transfer modes, among them streaming mode (MODE S) and   
extended block mode (MODE E). MODE E transfers now support keeping the data connection alive 
in between uploads, resulting in significantly higher throughput and providing better compatibility  
with the Globus transfer service.

### gplazma
The LDAP plugin is updated to natively support **auth** phase. As a result,
there is need for an extra mutator plugin to use ldap based authentication
```
    auth     optional   ldap
    map      sufficient ldap
```
NOTICE, that even it's not rewuired to use ldaps:// url, it STRONGLY avised to do so!

The configuration of LDAP-only-setups, which are becoming more important, is now much easier.
Previously, the plugins `jaas`, `mutator` and `ldap` were necessary for such a setup. Now, the 
`ldap` module is sufficient. An example of a new-style gplazma configuration file would be:

```
 auth     optional   ldap
 map      optional   ldap
 session  optional   ldap
 identity optional   ldap
```

This functionality relies on using either OpenJDK or Oracle JDK.

### History

### Hoppingmanager

### httpd

### Info

### NFS
New export option **lt=** to control layout types is added. The supported values are:
   - flex_files
   - nfsv4_1_files
While is't recommended to use flex_files layout type, only moder kernel version (and RHEL 7.5)
are supporting it. If kernel client capabilities are unknow, the use nfsv4_1_files:
  ```
  / old-hosts(rw,lt=nfsv4_1_files)
```

```
nfs.enable.kafka = true|false
nfs.kafka.bootstrap-servers= host1:port,host2:port
```

### Pinmanager

### PNFS Manager

### Pool
Added configuration option to publish billing records into apache-kafka queue:
```
pool.enable.kafka = true|false
pool.kafka.bootstrap-servers= host1:port,host2:port
```

### Poolmanager

### Resilience

In the various 4.0 releases, resilience was improved in a number ways.  We
summarize them here.

First, a number of fixes have made it more robust:  uncaught
exceptions should no longer allow tasks to get stuck in the queue, and
the logic concerning available file sources when clear cache location messages
has been fixed.  In addition, we have fixed the way fatal error alarms
are generated so as to avoid spamming the alarm service and domain logs
(there is now a special resilience log for recording these errors on a
file-by-file basis, configured as usual via properties and logback.xml).

Two features have been added to resilience which enhances its usability.
The first involves a redefinition of what pools it considers to be accessible
as opposed to usable (writable) for making copies.  With this change it is
now possible to close off a resilient pool from external writes while still
allowing resilience to use them, i.e., by setting

```
\s resilient-pool-1 pool disable -store
```

Resilience no long considers such a pool 'inaccessible'.  The rule is now:
a pool is readable (by external clients) unless its disabled flags include
'fetch', and is writable (by resilience) unless its disabled flags include
-p2p-client.

The second new feature is integration of staging into resilience. Files
which are CUSTODIAL ONLINE (i.e., resilient but also stored on tape)
will now be automatically restored from tape if all their current replicas
are not accessible (e.g., because all those pools have gone offline). Since
this is achieved via communication with the Pool Manager, we have made sure that
a resilient group whose pools are connected to tape (viz. for stores) but for
which staging has been disabled will nonetheless receive the first restored
replica via p2p from some other eligible staging pool selected by the Pool
Manager.

The default sorting order for the admin history and history errors commands
has been changed from descending to ascending (more user friendly).

Resilience has now been running for almost a year on a number of systems at
DESY and elsewhere with reliable and performant results.

### Space Manager

### SRM

### SRM Manager

### Statistics

### Topo

### Transfer Managers

### WebDAV

### XRootD

### Zookeeper
Updated external zookeeper libraries to version 3.4.11

### Changelog from 4.0.0 to 4.1.0

<!-- git log 4.0..4.1 -no-merges -format='[%h](https://github.com/dcache/dcache/commit/%H)%n:   %s%n' -->

(tbd as soon as the branching is done)



</div>
</div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js" type="text/javascript"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"
        integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>

<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js" type="text/javascript"></script>
